{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RLpart1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOO+vdtOtVocgPgim1PBp6T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbhishekRP2002/ML-AI-Notes-and-Self-Practice/blob/main/Reinforcement%20Learning%20/RLpart1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Frozen Lake Game using RL"
      ],
      "metadata": {
        "id": "zCbWtD8BJPuh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Winter is here. You and your friends were tossing around a frisbee at the park when you made a wild throw that left the frisbee out in the middle of the lake. The water is mostly frozen, but there are a few holes where the ice has melted. If you step into one of those holes, you'll fall into the freezing water. At this time, there's an international frisbee shortage, so it's absolutely imperative that you navigate across the lake and retrieve the disc. However, the ice is slippery, so you won't always move in the direction you intend. The surface is described using a grid like the following:"
      ],
      "metadata": {
        "id": "B9YMTBklJXXz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SFFF\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "FHFH\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "FFFH\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "HFFG"
      ],
      "metadata": {
        "id": "nb_dHWRtJZmV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This grid is our environment where S is the agent's starting point, and it's safe. F represents the frozen surface and is also safe. H represents a hole, and if our agent steps in a hole in the middle of a frozen lake, well, that's not good. Finally, G represents the goal, which is the space on the grid where the prized frisbee is located.\n",
        "\n",
        "The agent can navigate left, right, up, and down, and the episode ends when the agent reaches the goal or falls in a hole. It receives a reward of one if it reaches the goal, and zero otherwise."
      ],
      "metadata": {
        "id": "Sy9v_kTQJjNX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "WbXW9Wq1F_XA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gym \n",
        "import random \n",
        "import time "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output"
      ],
      "metadata": {
        "id": "vEhUMtsVHDAa"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating The Environment\n",
        "env = gym.make(\"FrozenLake-v0\")"
      ],
      "metadata": {
        "id": "6F0cojf6HIsN"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating The Q-Table\n",
        "action_space_size = env.action_space.n\n",
        "state_space_size = env.observation_space.n\n",
        "\n",
        "q_table = np.zeros((state_space_size, action_space_size))\n",
        "print(q_table)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJEK9UTIHmL3",
        "outputId": "5251b9ab-b2c4-4f23-f956-61d4edf10457"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Initializing the Q-learning parameters\n",
        "num_episodes=1000\n",
        "max_steps_per_episode = 100\n",
        "\n",
        "learning_rate = 0.1\n",
        "discount_rate = 0.99\n",
        "\n",
        "exploration_rate = 1\n",
        "max_exploration_rate =1\n",
        "min_exploration_rate = 0.01\n",
        "exploration_decay_rate = 0.01"
      ],
      "metadata": {
        "id": "ayk4WjxMH8qt"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rewards_all_episodes = []"
      ],
      "metadata": {
        "id": "lRVJsV3aI9z_"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q-learning algorithm\n",
        "for episode in range(num_episodes):\n",
        "    # initialize new episode params\n",
        "    state = env.reset()\n",
        "\n",
        "    done = False\n",
        "    rewards_current_episode = 0\n",
        "\n",
        "    for step in range(max_steps_per_episode): \n",
        "        # Exploration-exploitation trade-off\n",
        "        exploration_rate_threshold = random.uniform(0, 1)\n",
        "        if exploration_rate_threshold > exploration_rate:\n",
        "         action = np.argmax(q_table[state,:]) \n",
        "        else:\n",
        "         action = env.action_space.sample()\n",
        "        # Take new action\n",
        "        new_state, reward, done, info = env.step(action)\n",
        "        # Update Q-table\n",
        "        q_table[state, action] = q_table[state, action] * (1 - learning_rate) + \\\n",
        "         learning_rate * (reward + discount_rate * np.max(q_table[new_state, :]))\n",
        "        # Set new state\n",
        "        state = new_state\n",
        "        rewards_current_episode += reward \n",
        "        # Add new reward        \n",
        "        if done == True: \n",
        "           break\n",
        "    # Exploration rate decay   \n",
        "    exploration_rate = min_exploration_rate + \\\n",
        "    (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)\n",
        "    # Add current episode reward to total rewards list\n",
        "    rewards_all_episodes.append(rewards_current_episode)"
      ],
      "metadata": {
        "id": "J9CUc5uFKIQB"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate and print the average reward per thousand episodes\n",
        "rewards_per_thousand_episodes = np.split(np.array(rewards_all_episodes),num_episodes/1000)\n",
        "count = 1000"
      ],
      "metadata": {
        "id": "vpJOc9BGLrqZ"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"********Average reward per thousand episodes********\\n\")\n",
        "for r in rewards_per_thousand_episodes:\n",
        "    print(count, \": \", str(sum(r/1000)))\n",
        "    count += 1000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ys3iDy3NLtNS",
        "outputId": "ce6f5a57-1383-4bf0-8836-415c616956a5"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "********Average reward per thousand episodes********\n",
            "\n",
            "1000 :  0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print updated Q-table\n",
        "print(\"\\n\\n********Q-table********\\n\")\n",
        "print(q_table)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTczjiw1Lw9X",
        "outputId": "8ebf5865-83f9-4048-aae3-77284dc65f5d"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "********Q-table********\n",
            "\n",
            "[[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "QN2oFtAQNnUo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}